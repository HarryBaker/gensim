{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 1: Prepare LDA models for topic-diff estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-26 16:20:29,889 : INFO : check\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.info(\"check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Download dataset\n",
    "Dataset description: https://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File `dataset/docs.txt.gz' already there; not retrieving.\n",
      "File `dataset/vocab.txt' already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p dataset/\n",
    "!wget -O dataset/docs.txt.gz -nc https://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/docword.nytimes.txt.gz\n",
    "!wget -O dataset/vocab.txt -nc https://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/vocab.nytimes.txt\n",
    "!gunzip -k -f dataset/docs.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cleanup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-26 16:20:36,506 : INFO : Load dictionary\n",
      "2017-03-26 16:20:36,562 : INFO : Construct corpus\n",
      "2017-03-26 16:22:32,933 : INFO : Expand corpus\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Load dictionary\")\n",
    "with open(\"dataset/vocab.txt\") as infile:\n",
    "    words = [line.strip() for line in infile]\n",
    "id2word = dict(zip(range(1, len(words) + 1), words))\n",
    "\n",
    "logging.info(\"Construct corpus\")\n",
    "docs = {}\n",
    "with open(\"dataset/docs.txt\") as infile:\n",
    "    for _ in range(3):\n",
    "        next(infile)\n",
    "        \n",
    "    for line in infile:\n",
    "        d, wid, cnt = map(int, line.strip().split(\" \"))\n",
    "        docs.setdefault(d, {})\n",
    "        docs[d][id2word[wid]] = cnt\n",
    "        \n",
    "logging.info(\"Expand corpus\")\n",
    "documents = []\n",
    "for (_, val) in docs.iteritems():\n",
    "    curr_doc = []\n",
    "\n",
    "    for (w, cnt) in val.items():\n",
    "        for _ in range(cnt):\n",
    "            curr_doc.append(w)\n",
    "    documents.append(curr_doc)\n",
    "    \n",
    "docs.clear()\n",
    "id2word.clear()\n",
    "del docs, id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Split dataset to train and holdout \n",
    "\n",
    "reduce corpus size to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(documents)\n",
    "\n",
    "TRAINSET_SIZE = 100000\n",
    "HOLDOUT_SIZE = 20000\n",
    "\n",
    "trainset = documents[:TRAINSET_SIZE]\n",
    "holdout = documents[TRAINSET_SIZE:TRAINSET_SIZE + HOLDOUT_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Filter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(documents=trainset, prune_at=None)\n",
    "print(dictionary)\n",
    "\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.3, keep_n=None)\n",
    "dictionary.compactify()\n",
    "print(dictionary)\n",
    "\n",
    "!rm -rf models/*\n",
    "!mkdir -p models/\n",
    "dictionary.save(\"models/dictionary.corpora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Covert dataset to bag-of-word for LDA training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 dataset/trainset.json\n",
      "20000 dataset/holdout.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dataset/trainset.json\", 'w') as outfile:\n",
    "    for doc in trainset:\n",
    "        outfile.write(json.dumps({\"d2b\": dictionary.doc2bow(doc)}) + \"\\n\")\n",
    "        \n",
    "with open(\"dataset/holdout.json\", 'w') as outfile:\n",
    "    for doc in holdout:\n",
    "        outfile.write(json.dumps({\"d2b\": dictionary.doc2bow(doc)}) + \"\\n\")\n",
    "\n",
    "!wc -l dataset/trainset.json\n",
    "!wc -l dataset/holdout.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare stuff for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-26 16:24:42,918 : INFO : loading Dictionary object from models/dictionary.corpora\n",
      "2017-03-26 16:24:42,950 : INFO : loaded models/dictionary.corpora\n",
      "2017-03-26 16:24:42,963 : INFO : using symmetric alpha at 0.0133333333333\n",
      "2017-03-26 16:24:42,964 : INFO : using symmetric eta at 1.38112008839e-05\n",
      "2017-03-26 16:24:42,984 : INFO : using serial LDA version on this node\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "\n",
    "dictionary = Dictionary.load(\"models/dictionary.corpora\")\n",
    "lda = LdaMulticore(num_topics=75, id2word=dictionary, workers=4, eval_every=None,\n",
    "                   passes=10, batch=True, chunksize=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train model & dump model every 10k documents (+ save perplexity value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p models/lda\n",
    "\n",
    "BATCH_SIZE = 20000\n",
    "EPOCH_NUM = 3\n",
    "\n",
    "for ep in range(1, EPOCH_NUM + 1):\n",
    "    logging.info(\"---=== EPOCH #%d ===---\", ep)\n",
    "    \n",
    "    logging.info(\"Shuffle trainset\")\n",
    "    !shuf dataset/trainset.json > dataset/trainset_shuffled.json\n",
    "    \n",
    "    with open(\"dataset/trainset_shuffled.json\") as infile:\n",
    "        batch = []\n",
    "\n",
    "        for idx, line in enumerate(infile):\n",
    "            batch.append(json.loads(line)[\"d2b\"])\n",
    "\n",
    "            if (idx + 1) % BATCH_SIZE == 0:\n",
    "                logging.info(\"#%d\", idx + 1)\n",
    "                lda.update(batch)\n",
    "                lda.save(\"models/lda/ep{}_docs{}_lda.model\".format(ep, idx + 1))\n",
    "                batch[:] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Go to \"Part 2: Visualize topic-difference\" notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
